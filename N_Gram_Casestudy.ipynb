{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aca37ed",
   "metadata": {},
   "source": [
    "## N Grams : Feature Extraction method in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd273e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc289eb",
   "metadata": {},
   "source": [
    "#### Create data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1326b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Text' : ['This Pasta is tasty' , \n",
    "                             'This pasta is not tasty',\n",
    "                              'Pasta is delicious',\n",
    "                              'I like pasta very much',\n",
    "                              'Pasta is good but cost is little high' ,\n",
    "                              'Pasta taste is good and affordable',\n",
    "                              'Pasta taste is really nice however price is not affordable'] , \n",
    "                   'Output': [1,0,1,1,0,1,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e28935b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This Pasta is tasty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This pasta is not tasty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pasta is delicious</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I like pasta very much</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pasta is good but cost is little high</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pasta taste is good and affordable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pasta taste is really nice however price is no...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Output\n",
       "0                                This Pasta is tasty       1\n",
       "1                            This pasta is not tasty       0\n",
       "2                                 Pasta is delicious       1\n",
       "3                             I like pasta very much       1\n",
       "4              Pasta is good but cost is little high       0\n",
       "5                 Pasta taste is good and affordable       1\n",
       "6  Pasta taste is really nice however price is no...       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175bedc1",
   "metadata": {},
   "source": [
    "##### 1 means its positive sentence and 0 means its negative sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d27acfd",
   "metadata": {},
   "source": [
    "### Uni Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc46ee",
   "metadata": {},
   "source": [
    "#### Uni Grams (ngram_range = 1, 1) means its bag of words means extract feature on the single word only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e556fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a452b",
   "metadata": {},
   "source": [
    "#### Fit the data with countvectorizer(bow / uni grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08818a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_gram = cv1.fit_transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8343ce09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x21 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 38 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc94f58",
   "metadata": {},
   "source": [
    "#### Vocabulary in uni gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c24c4bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 19, 'pasta': 14, 'is': 8, 'tasty': 18, 'not': 13, 'delicious': 4, 'like': 9, 'very': 20, 'much': 11, 'good': 5, 'but': 2, 'cost': 3, 'little': 10, 'high': 6, 'taste': 17, 'and': 1, 'affordable': 0, 'really': 16, 'nice': 12, 'however': 7, 'price': 15}\n"
     ]
    }
   ],
   "source": [
    "print(cv1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f17c7d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv1.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28aba06",
   "metadata": {},
   "source": [
    "#### We get 21 vocabs from n gram in one corpus (2 documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81d6a1",
   "metadata": {},
   "source": [
    "#### metrics to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f485889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 0, 1, 1, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_gram.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfe4cb",
   "metadata": {},
   "source": [
    "### Uni Gram + Bi gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c32c7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(ngram_range = (1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b97788",
   "metadata": {},
   "source": [
    "#### Here we create features/ vocabs on uni gram (on single word) and Bi gram(on pair of word i.e. 2 consecutive words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cafdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_bi_gram = cv2.fit_transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da71e35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x46 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 71 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_bi_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503d09b",
   "metadata": {},
   "source": [
    "#### Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "414348ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 42, 'pasta': 31, 'is': 14, 'tasty': 41, 'this pasta': 43, 'pasta is': 32, 'is tasty': 20, 'not': 28, 'is not': 18, 'not tasty': 30, 'delicious': 7, 'is delicious': 15, 'like': 21, 'very': 44, 'much': 25, 'like pasta': 22, 'pasta very': 34, 'very much': 45, 'good': 8, 'but': 3, 'cost': 5, 'little': 23, 'high': 11, 'is good': 16, 'good but': 10, 'but cost': 4, 'cost is': 6, 'is little': 17, 'little high': 24, 'taste': 39, 'and': 1, 'affordable': 0, 'pasta taste': 33, 'taste is': 40, 'good and': 9, 'and affordable': 2, 'really': 37, 'nice': 26, 'however': 12, 'price': 35, 'is really': 19, 'really nice': 38, 'nice however': 27, 'however price': 13, 'price is': 36, 'not affordable': 29}\n"
     ]
    }
   ],
   "source": [
    "print(cv2.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f8c6125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv2.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad952e3",
   "metadata": {},
   "source": [
    "#### Here it create 46 vocabs.First on uni gram (on single word) and then Bi gram ( on pair of words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e4464",
   "metadata": {},
   "source": [
    "#### metrics to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb233f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_bi_gram.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b216e569",
   "metadata": {},
   "source": [
    "## Bi gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "427b6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv3 = CountVectorizer(ngram_range = (2 ,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb603dc",
   "metadata": {},
   "source": [
    "##### Here it will create features or vocab only on pair of words (2 consecutive words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7bf38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram = cv3.fit_transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e641923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x25 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 33 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fb8e9",
   "metadata": {},
   "source": [
    "#### Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b58c998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this pasta': 23, 'pasta is': 17, 'is tasty': 11, 'is not': 9, 'not tasty': 16, 'is delicious': 6, 'like pasta': 12, 'pasta very': 19, 'very much': 24, 'is good': 7, 'good but': 4, 'but cost': 1, 'cost is': 2, 'is little': 8, 'little high': 13, 'pasta taste': 18, 'taste is': 22, 'good and': 3, 'and affordable': 0, 'is really': 10, 'really nice': 21, 'nice however': 14, 'however price': 5, 'price is': 20, 'not affordable': 15}\n"
     ]
    }
   ],
   "source": [
    "print(cv3.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f6995f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv3.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e149c8",
   "metadata": {},
   "source": [
    "##### Here it built 25 features / vocabs only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551df9e2",
   "metadata": {},
   "source": [
    "###### metrics to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ff69a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1],\n",
       "       [0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce15d5a",
   "metadata": {},
   "source": [
    "### Uni gram + Bi gram + Tri Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5b33402",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv4 = CountVectorizer(ngram_range = (1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78702938",
   "metadata": {},
   "source": [
    "#### Here it create features or vocabs on single word and 2 cons word and 3 cons word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20f0e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_bi_tri_gram = cv4.fit_transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58b0a325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x70 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 97 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_bi_tri_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b29bb5",
   "metadata": {},
   "source": [
    "#### Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "513d16c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 65, 'pasta': 44, 'is': 19, 'tasty': 64, 'this pasta': 66, 'pasta is': 45, 'is tasty': 31, 'this pasta is': 67, 'pasta is tasty': 49, 'not': 41, 'is not': 26, 'not tasty': 43, 'pasta is not': 48, 'is not tasty': 28, 'delicious': 9, 'is delicious': 20, 'pasta is delicious': 46, 'like': 32, 'very': 68, 'much': 37, 'like pasta': 33, 'pasta very': 52, 'very much': 69, 'like pasta very': 34, 'pasta very much': 53, 'good': 10, 'but': 3, 'cost': 6, 'little': 35, 'high': 15, 'is good': 21, 'good but': 13, 'but cost': 4, 'cost is': 7, 'is little': 24, 'little high': 36, 'pasta is good': 47, 'is good but': 23, 'good but cost': 14, 'but cost is': 5, 'cost is little': 8, 'is little high': 25, 'taste': 60, 'and': 1, 'affordable': 0, 'pasta taste': 50, 'taste is': 61, 'good and': 11, 'and affordable': 2, 'pasta taste is': 51, 'taste is good': 62, 'is good and': 22, 'good and affordable': 12, 'really': 57, 'nice': 38, 'however': 16, 'price': 54, 'is really': 29, 'really nice': 58, 'nice however': 39, 'however price': 17, 'price is': 55, 'not affordable': 42, 'taste is really': 63, 'is really nice': 30, 'really nice however': 59, 'nice however price': 40, 'however price is': 18, 'price is not': 56, 'is not affordable': 27}\n"
     ]
    }
   ],
   "source": [
    "print(cv4.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c7da32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv4.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef8ac1b",
   "metadata": {},
   "source": [
    "##### Here it create 70 vocabs ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7776b2",
   "metadata": {},
   "source": [
    "#### metrics to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcbe1ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 2, 0, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_bi_tri_gram.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f9761",
   "metadata": {},
   "source": [
    "### Tri Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60efcde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv5 = CountVectorizer(ngram_range = (3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665413c9",
   "metadata": {},
   "source": [
    "#### Here it craete vocabs or feature on 3 cons words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4558aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_gram = cv5.fit_transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e647272d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x24 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d091a",
   "metadata": {},
   "source": [
    "#### metrics to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03f86775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this pasta is': 23, 'pasta is tasty': 16, 'pasta is not': 15, 'is not tasty': 9, 'pasta is delicious': 13, 'like pasta very': 11, 'pasta very much': 18, 'pasta is good': 14, 'is good but': 6, 'good but cost': 3, 'but cost is': 0, 'cost is little': 1, 'is little high': 7, 'pasta taste is': 17, 'taste is good': 21, 'is good and': 5, 'good and affordable': 2, 'taste is really': 22, 'is really nice': 10, 'really nice however': 20, 'nice however price': 12, 'however price is': 4, 'price is not': 19, 'is not affordable': 8}\n"
     ]
    }
   ],
   "source": [
    "print(cv5.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a724552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv5.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de97ae",
   "metadata": {},
   "source": [
    "#### It create only 24 vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e476118",
   "metadata": {},
   "source": [
    "#### metrics to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6be15017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_gram.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d402524c",
   "metadata": {},
   "source": [
    "##### If remove stop words then also its meaning is not change or not loss of meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32cf6d2",
   "metadata": {},
   "source": [
    "#### But N grams having some Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbbc95c",
   "metadata": {},
   "source": [
    "#### 1.Sparsity when large data (We noticed as compare to 2 sentence this having large sparse matrix)\n",
    "#### 2.It not able to tell which word is significant \n",
    "#### 3.oov (out of vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe154ad",
   "metadata": {},
   "source": [
    "### To avoid this things TF-IDF is use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad861144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
